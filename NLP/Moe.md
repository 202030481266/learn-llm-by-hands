## 官方论文

[DeepSeek-V3 Technical Report](https://arxiv.org/pdf/2412.19437)

## 参数解释

```python
@dataclass
class ModelArgs:
    max_batch_size: int = 8
    max_seq_len: int = 4096 * 4
    dtype: Literal["bf16", "fp8"] = "bf16"
    vocab_size: int = 102400
    dim: int = 2048
    inter_dim: int = 10944
    moe_inter_dim: int = 1408
    n_layers: int = 27
    n_dense_layers: int = 1
    n_heads: int = 16
    # moe
    n_routed_experts: int = 64
    n_shared_experts: int = 2
    n_activated_experts: int = 6
    n_expert_groups: int = 1
    n_limited_groups: int = 1
    score_func: Literal["softmax", "sigmoid"] = "softmax"
    route_scale: float = 1.
    # mla
    q_lora_rank: int = 0
    kv_lora_rank: int = 512
    qk_nope_head_dim: int = 128
    qk_rope_head_dim: int = 64
    v_head_dim: int = 128
    # yarn
    original_seq_len: int = 4096
    rope_theta: float = 10000.0
    rope_factor: float = 40
    beta_fast: int = 32
    beta_slow: int = 1
    mscale: float = 1.
```

MoE 是一种模型架构技术，它允许模型在保持计算成本相对可控的情况下，极大地增加模型参数量。其核心思想是：对于每个输入（例如一个 token），不是所有参数都参与计算，而是通过一个“门控网络”（Gating Network）来选择性地激活一小部分“专家网络”（Expert Networks）。

以下是 `ModelArgs` 中 MoE 相关参数的中文解释：

1.  **`moe_inter_dim` (MoE 中间层维度):**
    *   **解释:** 这个参数定义了 *每个* MoE 层中 *单个专家网络* 的前馈网络（FFN）的中间层维度。
    *   **对比:** 普通的 Transformer FFN 层有一个中间层维度 `inter_dim` (这里是 10944)。而在 MoE 层中，每个专家都有自己的 FFN，其内部维度由 `moe_inter_dim` (这里是 1408) 指定。通常 `moe_inter_dim` 会远小于 `inter_dim`，因为模型的总容量来自于大量专家的集合，而不是单个巨大的 FFN。
    *   **作用:** 控制单个专家的复杂度和参数量。

2.  **`n_routed_experts` (路由专家数量):**
    *   **解释:** 指在每个 MoE 层中，可供门控网络选择的“路由专家”的总数量。这些是模型学习到的专门化子网络。
    *   **作用:** 代表了模型在该层提供的“专业知识”的多样性。总共有 64 个这样的专家可供选择。

3.  **`n_shared_experts` (共享专家数量):**
    *   **解释:** 指在每个 MoE 层中，*所有* token 都会经过处理的“共享专家”的数量。这些专家不通过门控网络选择，而是强制所有数据流都通过它们。
    *   **作用:** 可能用于学习一些通用的、对所有 token 都很重要的特征或转换，作为路由专家的补充。这里设置了 2 个共享专家。

4.  **`n_activated_experts` (激活专家数量):**
    *   **解释:** 这是 MoE 的关键参数之一。它指定了对于*每个*输入 token，门控网络会从 `n_routed_experts` 个路由专家中选择并*激活*多少个专家来处理该 token。这通常采用 Top-K 路由策略，即选择得分最高的 K 个专家。
    *   **作用:** 控制了每个 token 在 MoE 层实际使用的计算量。这里设置为 6，意味着每个 token 的计算会涉及门控网络选出的 6 个得分最高的路由专家（以及 `n_shared_experts` 个共享专家）。这远小于总的路由专家数 64，体现了 MoE 的稀疏激活特性。

5.  **`n_expert_groups` (专家组数量):**
    *   **解释:** 这个参数可能用于将 `n_routed_experts` 分成几个组，可能与特定的并行化策略（如 Expert Parallelism）或路由机制的设计有关。
    *   **作用:** 如果大于 1，可能表示专家被划分到不同的设备或处理单元上，或者路由是在组级别进行的。在这里，值为 1 通常意味着没有进行额外的分组，所有路由专家属于一个逻辑整体。

6.  **`n_limited_groups` (限制组数量):**
    *   **解释:** 这个参数不太常见，可能与更高级的路由策略有关，例如限制一个 token 只能从特定的几个组（如果 `n_expert_groups` > 1）中选择专家，或者与负载均衡机制有关，确保 token 分布到不同组的专家上。
    *   **作用:** 用于对路由过程施加额外的约束。在这里，值为 1 且 `n_expert_groups` 也为 1，表明在这个配置下可能没有启用复杂的组限制路由策略。

7.  **`score_func` (路由评分函数):**
    *   **解释:** 指定门控网络计算每个 token 与每个路由专家“亲和度”得分后，使用的归一化或转换函数。
    *   **选项:**
        *   `softmax`: 将所有路由专家的得分进行 Softmax 归一化，使得所有得分之和为 1，形成一个概率分布。这是最常见的选择，尤其适用于 Top-K 路由，权重可以解释为将 token 分配给该专家的概率或贡献度。
        *   `sigmoid`: 对每个专家的得分独立应用 Sigmoid 函数，将得分映射到 (0, 1) 区间。这种方式下，得分不直接构成和为 1 的分布，可能用于不同的路由机制或解释。
    *   **作用:** 决定了如何从原始的门控网络输出（logits）生成用于路由决策和加权组合专家输出的权重。DeepSeek 这里使用了 `softmax`。

8.  **`route_scale` (路由缩放因子):**
    *   **解释:** 在将门控网络输出的 logits 输入到 `score_func` (如 Softmax) 之前，对其进行缩放的因子。
    *   **作用:** 控制路由决策的“尖锐度”或“置信度”。较大的 `route_scale` 会使得 Softmax 输出更集中在得分最高的几个专家上（更尖锐），较小的值则会使分布更平缓。可以影响训练动态和专家特化的程度。这里设置为 1.0，表示没有应用额外的缩放。

**总结:**

这些 MoE 参数共同定义了 DeepSeek 模型中混合专家层的结构和行为方式：
*   模型在特定层（非所有层，由 `n_layers` 和 `n_dense_layers` 的关系暗示哪些层是 MoE 层）使用了 MoE 结构。
*   每个 MoE 层包含 64 个路由专家和 2 个共享专家。
*   每个路由专家内部是一个 FFN，中间维度为 1408。
*   对于每个 token，会通过 Softmax 计算路由权重，并选择得分最高的 6 个路由专家进行计算，同时也会通过 2 个共享专家。
*   专家没有被显式地分成多个组进行特殊处理（`n_expert_groups=1`, `n_limited_groups=1`）。
*   路由的 logits 在进入 Softmax 前没有进行额外的缩放（`route_scale=1.0`）。


## 专家中的门控信号

```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.w2(F.silu(self.w1(x)) * self.w3(x))
```

这个 `w3` 层是关键。

想象一下，这个 `Expert` 模块就像一个小型的计算单元，它负责处理输入信息 `x` 并产生一个输出。

1.  **`w1` 和 `w2` 的作用（标准流程）：**
    *   通常，在类似这样的模块里，会有一个 `w1` 层，它的作用是把输入的维度 `dim` 扩大到一个中间维度 `inter_dim`。这就像是先把信息“摊开”，让网络有更多的空间去分析和处理。
    *   然后会有一个 `w2` 层，它的作用是把处理后的中间维度 `inter_dim` 再“压缩”回原来的维度 `dim`，得到最终的输出。
    *   中间通常还会有一个激活函数（这里是 `F.silu`），用来增加非线性，让网络能学习更复杂的关系。如果只有 `w1` 和 `w2`（以及激活函数），那就是一个标准的、简单的前馈网络（Feed-Forward Network, FFN）结构。

2.  **`w3` 的作用（引入门控机制）：**
    *   现在我们看代码里的计算：`self.w2(F.silu(self.w1(x)) * self.w3(x))`
    *   注意看 `*` 这个乘法符号。这里不是简单的把 `w1` 处理后的结果直接传给 `w2`。
    *   代码兵分两路：
        *   **第一路:** 输入 `x` 先经过 `w1` 处理，然后通过 `F.silu` 激活函数。得到一个结果（维度是 `inter_dim`）。
        *   **第二路:** 输入 `x` *同时*也经过 `w3` 处理。`w3` 也把 `x` 从 `dim` 转换到 `inter_dim` 维度。得到另一个结果。
    *   **关键步骤：** 这两路得到的结果（维度都是 `inter_dim`）会进行**逐元素相乘 (`*`)**。
    *   **`w3` 的角色：** `w3(x)` 的输出就像一个“**门控（gate）**”或者“**开关**”。它会根据原始输入 `x` 动态地决定，第一路 (`F.silu(self.w1(x))`) 算出来的信息中，哪些部分应该被“放大”（乘以一个较大的数），哪些部分应该被“缩小”甚至“关闭”（乘以一个接近 0 的数）。
    *   **简单比喻：** 想象 `F.silu(self.w1(x))` 是主要的信号流，而 `w3(x)` 是一个调节器。这个调节器（`w3`）看着原始输入 `x`，然后决定如何调整（通过乘法）主信号流里的每一个细节，让最终传递给 `w2` 的信息更有用。

3.  **为什么需要 `w3`？**
    *   这种带有“门控”的设计（这里具体叫 SwiGLU 结构）被发现通常比简单的 FFN（只有 `w1` 和 `w2` 加激活函数）效果更好。
    *   它给了网络一种更灵活的方式来控制信息流动。网络可以学习到：对于当前的输入 `x`，`w1` 提取出的特征中，哪些是真正重要的，需要被保留和加强；哪些是不太重要的，需要被抑制。
    *   这种精细的控制有助于提升模型的性能和学习能力。

**总结一下 `w3` 的作用：**

`w3` 与 `w1` 并行处理输入 `x`。`w3` 的输出结果通过与 `w1`（激活后）的输出结果进行**乘法**运算，起到一个**动态门控**的作用，它可以根据输入 `x` **选择性地调整** `w1` 路径传递的信息流，让更有用的信息传递给下一层 `w2`，从而可能获得更好的模型效果。它不是一个简单的线性层，而是构成了一个更复杂的、带有门控机制的计算单元。
