# LoRA (Low-Rank Adaptation) ä½ç§©é€‚åº”

## ğŸ“š è®ºæ–‡ä¿¡æ¯

- **è®ºæ–‡**: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- **ä½œè€…**: Edward J. Hu, Yelong Shen, Phillip Wallis, et al. (Microsoft)
- **å‘å¸ƒæ—¶é—´**: 2021å¹´6æœˆ

## ğŸ¯ æ ¸å¿ƒæ€æƒ³

LoRAçš„æ ¸å¿ƒæ´å¯Ÿæ˜¯ï¼š**é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡æ›´æ–°å…·æœ‰ä½"å†…åœ¨ç§©"ï¼ˆlow intrinsic rankï¼‰**ã€‚

ä¼ ç»Ÿå¾®è°ƒéœ€è¦æ›´æ–°å®Œæ•´çš„æƒé‡çŸ©é˜µ $W \in \mathbb{R}^{d \times k}$ï¼Œå‚æ•°é‡å·¨å¤§ã€‚LoRAæå‡ºç”¨ä½ç§©åˆ†è§£æ¥è¿‘ä¼¼æƒé‡æ›´æ–°ï¼š

$$W' = W + \Delta W = W + BA$$

å…¶ä¸­ï¼š
- $W$: åŸå§‹é¢„è®­ç»ƒæƒé‡ï¼Œ**å†»ç»“ä¸æ›´æ–°**
- $B \in \mathbb{R}^{d \times r}$: ä½ç§©çŸ©é˜µ
- $A \in \mathbb{R}^{r \times k}$: ä½ç§©çŸ©é˜µ  
- $r \ll \min(d, k)$: ç§©ï¼ˆé€šå¸¸ $r = 4, 8, 16, 32$ï¼‰

## ğŸ” æ•°å­¦åŸç†

### å‰å‘ä¼ æ’­

å¯¹äºè¾“å…¥ $x$ï¼ŒåŸå§‹çº¿æ€§å±‚è¾“å‡ºä¸ºï¼š
$$h = Wx$$

æ·»åŠ LoRAåï¼š
$$h = Wx + \Delta Wx = Wx + BAx = (W + BA)x$$

ä¸ºäº†æ§åˆ¶æ›´æ–°å¹…åº¦ï¼Œå¼•å…¥ç¼©æ”¾å› å­ $\alpha$ï¼š
$$h = Wx + \frac{\alpha}{r}BAx$$

### å‚æ•°åˆå§‹åŒ–

- **AçŸ©é˜µ**: ä½¿ç”¨Kaiming/Heåˆå§‹åŒ–ï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰
- **BçŸ©é˜µ**: **åˆå§‹åŒ–ä¸ºé›¶**

è¿™æ ·ç¡®ä¿è®­ç»ƒå¼€å§‹æ—¶ $\Delta W = BA = 0$ï¼Œä¸æ”¹å˜åŸå§‹æ¨¡å‹çš„è¡Œä¸ºã€‚

### å‚æ•°é‡å¯¹æ¯”

| æ–¹æ³• | å‚æ•°é‡ | ç¤ºä¾‹ (d=4096, k=4096) |
|------|--------|----------------------|
| å…¨é‡å¾®è°ƒ | $d \times k$ | 16.8M |
| LoRA (r=8) | $r \times (d + k)$ | 65.5K |
| LoRA (r=16) | $r \times (d + k)$ | 131K |
| LoRA (r=32) | $r \times (d + k)$ | 262K |

**èŠ‚çœæ¯”ä¾‹**: ä½¿ç”¨ $r=8$ æ—¶ï¼Œå‚æ•°é‡å‡å°‘ **99.6%**ï¼

## ğŸ—ï¸ æ¨¡å—ç»“æ„

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   è¾“å…¥ x        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                 â”‚                 â”‚
           â–¼                 â”‚                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  å†»ç»“çš„ W    â”‚         â”‚         â”‚   A (rÃ—k)    â”‚ â† å¯è®­ç»ƒ
    â”‚  (dÃ—k)       â”‚         â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                â”‚
           â”‚                 â”‚                â–¼
           â”‚                 â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                 â”‚         â”‚   B (dÃ—r)    â”‚ â† å¯è®­ç»ƒ
           â”‚                 â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                 â”‚                â”‚
           â”‚                 â”‚                â”‚ Ã— Î±/r
           â”‚                 â”‚                â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚      ç›¸åŠ        â”‚
                    â–¼                 â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      è¾“å‡º h = Wx + BAx  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ ä»£ç å®ç°

### æ ¸å¿ƒLoRAçº¿æ€§å±‚

```python
class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, r=8, lora_alpha=16):
        super().__init__()
        # åŸå§‹æƒé‡ï¼ˆå†»ç»“ï¼‰
        self.weight = nn.Parameter(torch.empty(out_features, in_features))
        self.weight.requires_grad = False
        
        # LoRAå‚æ•°
        self.lora_A = nn.Parameter(torch.empty(r, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, r))  # åˆå§‹åŒ–ä¸º0
        
        self.scaling = lora_alpha / r
        
        # åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.lora_A)
    
    def forward(self, x):
        # åŸå§‹è¾“å‡º + LoRAå¢é‡
        result = F.linear(x, self.weight)
        result += (x @ self.lora_A.T @ self.lora_B.T) * self.scaling
        return result
```

### æƒé‡åˆå¹¶ï¼ˆæ¨ç†ä¼˜åŒ–ï¼‰

è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥å°†LoRAæƒé‡åˆå¹¶åˆ°åŸå§‹æƒé‡ä¸­ï¼Œ**æ¨ç†æ—¶æ— é¢å¤–å¼€é”€**ï¼š

```python
def merge(self):
    """W' = W + scaling * BA"""
    self.weight.data += self.scaling * (self.lora_B @ self.lora_A)

def unmerge(self):
    """W = W' - scaling * BA"""
    self.weight.data -= self.scaling * (self.lora_B @ self.lora_A)
```

## ğŸ® åº”ç”¨åœºæ™¯

### åœ¨Attentionä¸­åº”ç”¨LoRA

é€šå¸¸åªå¯¹ **Query (Q)** å’Œ **Value (V)** æŠ•å½±æ·»åŠ LoRAï¼š

```python
# å…¸å‹é…ç½®
lora_targets = ['q', 'v']  # åªå¯¹Qå’ŒVæ·»åŠ LoRA

# QæŠ•å½±
self.wq = LoRALinear(dim, dim, r=8, lora_alpha=16)
# VæŠ•å½±  
self.wv = LoRALinear(dim, dim, r=8, lora_alpha=16)
# Kå’ŒOä¿æŒåŸå§‹
self.wk = nn.Linear(dim, dim)
self.wo = nn.Linear(dim, dim)
```

**ä¸ºä»€ä¹ˆé€‰æ‹©Qå’ŒVï¼Ÿ**
- åŸè®ºæ–‡å®éªŒè¡¨æ˜ï¼Œåªè®­ç»ƒQå’ŒVæ•ˆæœæœ€å¥½
- KæŠ•å½±å¯¹æ€§èƒ½å½±å“è¾ƒå°
- å‡å°‘äº†ä¸€åŠçš„LoRAå‚æ•°

## ğŸ”§ è¶…å‚æ•°é€‰æ‹©

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| **r** | 8-64 | ç§©è¶Šå¤§ï¼Œè¡¨è¾¾èƒ½åŠ›è¶Šå¼ºï¼Œä½†å‚æ•°è¶Šå¤š |
| **lora_alpha** | 16-32 | é€šå¸¸è®¾ä¸º $2r$ æˆ–å›ºå®šå€¼ |
| **lora_dropout** | 0.05-0.1 | é˜²æ­¢è¿‡æ‹Ÿåˆ |
| **target_modules** | ["q", "v"] | å»ºè®®è‡³å°‘åŒ…å«Qå’ŒV |

### ä¸åŒä»»åŠ¡çš„æ¨èé…ç½®

```python
# ç®€å•ä»»åŠ¡ï¼ˆæ–‡æœ¬åˆ†ç±»ï¼‰
LoRAConfig(r=4, lora_alpha=8)

# ä¸­ç­‰ä»»åŠ¡ï¼ˆæŒ‡ä»¤å¾®è°ƒï¼‰
LoRAConfig(r=8, lora_alpha=16)

# å¤æ‚ä»»åŠ¡ï¼ˆå¤šä»»åŠ¡å­¦ä¹ ï¼‰
LoRAConfig(r=16, lora_alpha=32)

# æœ€å¤§è¡¨è¾¾èƒ½åŠ›
LoRAConfig(r=64, lora_alpha=128)
```

## ğŸš€ LoRAå˜ä½“

### 1. QLoRA (Quantized LoRA)

ç»“åˆ4-bité‡åŒ–ï¼Œè¿›ä¸€æ­¥å‡å°‘æ˜¾å­˜ï¼š

```python
# åŸºç¡€æƒé‡: 4-bité‡åŒ–å­˜å‚¨
# LoRAå‚æ•°: fp16/bf16å…¨ç²¾åº¦
Y = Dequantize(W_quant)Â·X + BAÂ·X Ã— scaling
```

**æ˜¾å­˜èŠ‚çœ**: åœ¨7Bæ¨¡å‹ä¸Šå¯å°†å¾®è°ƒæ˜¾å­˜ä»>70GBé™è‡³~6GB

### 2. LoRA+

æ”¹è¿›çš„ä¼˜åŒ–ç­–ç•¥ï¼š
- AçŸ©é˜µä½¿ç”¨è¾ƒå¤§å­¦ä¹ ç‡
- BçŸ©é˜µä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡

### 3. DoRA (Weight-Decomposed LoRA)

åˆ†è§£æƒé‡çš„magnitudeå’Œdirectionï¼š
$$W' = m \frac{W + BA}{\|W + BA\|}$$

### 4. AdaLoRA

è‡ªé€‚åº”è°ƒæ•´ä¸åŒå±‚çš„ç§© $r$ã€‚

## ğŸ“Š å®éªŒæ•ˆæœ

åœ¨LLaMA-7Bä¸Šçš„å…¸å‹è¡¨ç°ï¼š

| æ–¹æ³• | å¯è®­ç»ƒå‚æ•° | æ˜¾å­˜ | æ•ˆæœ |
|------|-----------|------|------|
| å…¨é‡å¾®è°ƒ | 7B | ~120GB | 100% |
| LoRA (r=8) | 4.2M | ~18GB | 97% |
| LoRA (r=16) | 8.4M | ~20GB | 98% |
| QLoRA (r=8) | 4.2M | ~6GB | 95% |

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. è®­ç»ƒæŠ€å·§

```python
# åªè®­ç»ƒLoRAå‚æ•°
for name, param in model.named_parameters():
    if 'lora_' not in name:
        param.requires_grad = False

# ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
optimizer = AdamW(model.parameters(), lr=1e-4)

# æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
```

### 2. å¤šLoRAåˆ‡æ¢

```python
# ä¿å­˜LoRAæƒé‡
lora_state = get_lora_state_dict(model)
torch.save(lora_state, "lora_adapter.pt")

# åŠ è½½ä¸åŒçš„LoRAé€‚é…å™¨
lora_chinese = torch.load("lora_chinese.pt")
lora_code = torch.load("lora_code.pt")

# è¿è¡Œæ—¶åˆ‡æ¢
set_lora_state_dict(model, lora_chinese)  # ä¸­æ–‡å¯¹è¯
set_lora_state_dict(model, lora_code)     # ä»£ç ç”Ÿæˆ
```

### 3. æ¨ç†ä¼˜åŒ–

```python
# è®­ç»ƒå®Œæˆååˆå¹¶æƒé‡
model.merge_lora()

# æ­¤æ—¶æ¨ç†ä¸åŸå§‹æ¨¡å‹å®Œå…¨ç›¸åŒ
# æ— é¢å¤–è®¡ç®—å¼€é”€ï¼
output = model(input)
```

## ğŸ”— ç›¸å…³å·¥ä½œ

- **Adapter**: åœ¨å±‚ä¹‹é—´æ’å…¥å°å‹ç½‘ç»œ
- **Prefix-Tuning**: åœ¨è¾“å…¥å‰æ·»åŠ å¯å­¦ä¹ çš„å‰ç¼€
- **Prompt-Tuning**: åªè®­ç»ƒè½¯æç¤ºå‘é‡
- **BitFit**: åªè®­ç»ƒåç½®é¡¹

LoRAç›¸æ¯”è¿™äº›æ–¹æ³•çš„ä¼˜åŠ¿ï¼š
- âœ… æ— æ¨ç†å»¶è¿Ÿï¼ˆå¯åˆå¹¶ï¼‰
- âœ… å‚æ•°æ•ˆç‡é«˜
- âœ… æ˜“äºåˆ‡æ¢å’Œç»„åˆ
- âœ… ä¸é‡åŒ–æŠ€æœ¯å…¼å®¹

## ğŸ“– å‚è€ƒèµ„æ–™

1. [LoRAåŸè®ºæ–‡](https://arxiv.org/abs/2106.09685)
2. [QLoRAè®ºæ–‡](https://arxiv.org/abs/2305.14314)
3. [PEFTåº“ (Hugging Face)](https://github.com/huggingface/peft)
4. [LLaMA-Adapter](https://arxiv.org/abs/2303.16199)

